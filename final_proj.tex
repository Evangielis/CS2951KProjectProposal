\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage[margin=3cm]{geometry}
\usepackage{courier}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.1in}


\begin{document}
\title{Emotion through Intel RealSense\\ \vspace{2 mm} {\large CS2951K Final Project Proposal}\\ {\large Ning Hou (nhou), Lee Painton, Eric Rosen}}

\maketitle
%\begin{center}Team member: Ning Hou (nhou)\end{center}

\section{Research question}
Affect display is the combination of facial, gestural and vocal cues by which persons consciously or unconsciously communicate emotion.  Cues such as facial expression, vocal prosody and gestural display are all modes by which the affective state of an individual can be inferred.  We are interested in exploring the efficacy of a multimodal Bayes filter when used to filter combined input from various modes into a probability whether a given subject is confused or not.  By confusion we mean the common definition where, given a situation, a person either understands it or is confused.  It is our contention that a multimodal approach will be much more effective than a single mode by itself.

\section{Significance}
Reliably determining user affect is an open problem in HCI and part of a field called affective computing.  The development of affect sensitive intelligent agents would computers to interact more effectively with humans in tasks where emotion has an impact, learning or driving for example.  Confusion is especially significant during these tasks as it can actively interfere, or even be dangerous in the case of tasks such as driving.  Confusion serves intuitively as a natural perceptual feedback respresenting the efficacy of an intelligent agents communication attempts and could be incorporated as part of the reward function in a learning agent.  It is our more immediate hope that we can utilize work in this project to make Baxter aware of confusion in subjects with whom he is interacting.

%\begin{itemize}
%\item 
%For example, strong negative emotion would trigger error detection/correction mechanism; faster convergence in computation to give back prompt feedback; alternative strategy in search space or algorithm for the solution.
%\item Alarm and report emergency for drivers, employees and patients.
%\item Improve user experience. The robots are more than our workers; they can be truly thoughtful and understanding companions.
%\end{itemize}

\section{Methodology}
%\subsection{How to solve the problem?}
We plan to formulate the problem at the highest level as a multimodal Bayes filter which returns a distribution on $P(confused | U)$ where $U$ is an evidence set of affective cues.  We plan initially focus on facial expression data, but given a relatively low accuracy reported from isolating modes (cite here) we feel it will be important to combine data from as many cues as possible.  Our corporae will be gathered by means of an automated quiz where a user sits down in front of a RealSense enabled computer and takes a short quiz of 10 questions designed to elicit both confusion and certainty.  After the quiz the subject will be asked to rate their experience with each question on a scale so as to establish a baseline from which we can derive ground truth.  Data from the RealSense device and end surveys will be recorded into a database during these quizzes.\\ \\

\section{Results}
Data from testing will be processed through the Bayes filter with different combinations of modes active.  This will be compared against our collected ground truth data for each subject as a way of determining accuracy of the results.

(table here)

%\begin{enumerate}
%\item Examine existing corporae to determine features which are representative of a confused state.
%\item Setup recording on I
%\item Solve the decision problem using Bayes filter and neural network.
%\item Test the model by user input using data set.
%\item Adjust and formalize the model based on results.
%\end{enumerate}

%\subsection{How to know if we have solved the problem?}
%\emph{Milestone 1.} Successful detection of user facial expression using Intel RealSense camera and based on that, inference of emotion.
%\emph{Milestone 2.} Formalized POMDP model that outputs decision from input user expression and inferred emotion.
%\emph{Milestone 3.} Extensive testing (hoping) to justify the performance of Intel RealSense emtion inference, our POMDP model and the output decision.
%\emph{Final results.} We have solved the problem if testing indicates that we can make satisfiable decision given user input facial expression and commands. 
%\section{Related Work}

\section{Related Work}
Kapoor et al \cite{kapoor2001towards} describes a theoretic framework used to describe affective states.  Our work borrows from this idea but is generalized rather than focused on the activity of learning.

\newpage
\section{Schedule}

\begin{center}
\begin{tabular}{ c | c }
\bf{Date} & \bf{TODO} \\ \hline
2/26 - 3/5 & Finalize theoretic framework and experiment design\\
3/6 - 3/12 & Program initial models and test with false data\\
3/13 - 3/19 & Design interview script and post interview survey, find interview subjects and schedule\\
3/20 - 3/26 & Have at least 10 subjects interviewed with collected data or move to backup plan\\
3/27 - 4/2 & Test data on models and compare\\
4/2 - 4/7 & Checkpoint presentation\\
4/8 - 4/14 & Collect more data as needed; tweak models\\
4/15 - 4/21 & Prepare results\\
4/23 - 4/28 & Final presentation
\end{tabular}
\end{center}

\section{Responsibilities}
Everyone will have involvement with all parts of the project.  We will assign work based on what is needed to meet milestones.

%\section{Bibliography}
\bibliographystyle{plain}
\bibliography{final_proj}

\end{document}
